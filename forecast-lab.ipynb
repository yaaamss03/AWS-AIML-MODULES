{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Student Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lab, you will prepare a dataset for creating a forecast by using Amazon Forecast.\n",
    "\n",
    "This lab includes two Jupyter notebooks:\n",
    "\n",
    "1. This notebook contains the steps that you will follow to prepare the dataset and evaluate the forecast.\n",
    "2. The `forecast-autorun.ipynb` notebook contains the steps to create the forecast by using Amazon Forecast. This notebook is run in the background when the lab starts, and it can take between 1–2 hours to complete. You will refer to this notebook during the lab steps, but you won't need to run any cells.\n",
    "\n",
    "\n",
    "## About the dataset\n",
    "\n",
    "This [Online Retail II](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II) dataset contains all transactions that occurred between January 12, 2009 and September 12, 2011 for a non-store, online retail organization that's registered and based in the United Kingdom. The company mainly sells unique all-occasion giftware. Many customers of the company are wholesalers.\n",
    "\n",
    "\n",
    "## Attribute information\n",
    "\n",
    "- **InvoiceNo** – Invoice number. Nominal. A 6-digit integral number that's uniquely assigned to each transaction. If this code starts with the letter *c*, it indicates a cancelation.\n",
    "- **StockCode** – Product (item) code. Nominal. A 5-digit integral number that's uniquely assigned to each distinct product.\n",
    "- **Description** – Product (item) name. Nominal.\n",
    "- **Quantity** – The quantities of each product (item) per transaction. Numeric.\n",
    "- **InvoiceDate** – Invoice date and time. Numeric. The day and time when a transaction was generated.\n",
    "- **UnitPrice** – Unit price. Numeric. Product price per unit in pounds sterling (£).\n",
    "- **CustomerID** – Customer number. Nominal. A 5-digit integral number that's uniquely assigned to each customer.\n",
    "- **Country** – Country name. Nominal. The name of the country where a customer resides.\n",
    "\n",
    "\n",
    "## Dataset attributions\n",
    "\n",
    "This dataset was obtained from:\n",
    "Dua, D. and Graff, C. (2019). UCI Machine Learning Repository (http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab instructions\n",
    "\n",
    "To complete this lab, read and run the cells below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Importing Python packages\n",
    "\n",
    "Start by importing the Python packages that you need.\n",
    "\n",
    "In the following code:\n",
    "\n",
    "- *boto3* represents the AWS SDK for Python (Boto3), which is the Python library for AWS\n",
    "- *pandas* provides DataFrames for manipulating time series data\n",
    "- *matplotlib* provides plotting functions\n",
    "- *sagemaker* represents the API that's needed to work with Amazon SageMaker\n",
    "- *time*, *sys*, *os*, *io*, and *json* provide helper functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "bucket_name='c86983a1854842l4354243t1w082980862-forecastbucket-1ingm7fkvpm8k'\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "import time, sys, os, io, json\n",
    "import xlrd\n",
    "!pip3 install pandas==1.5.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Exploring the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in the *Microsoft Excel* format. pandas can read Excel files.\n",
    "\n",
    "**Note:** This data might take 1–2 minutes to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = pd.read_excel('online_retail_II.xlsx',engine='openpyxl')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the description for the dataset, some values are missing. To keep things simple, you will remove anything wtih a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = retail.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by examining the data.\n",
    "\n",
    "How many rows and columns are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon Forecast has schemas for domains such as retail. Review the schema information at [RETAIL Domain](https://docs.aws.amazon.com/forecast/latest/dg/retail-domain.html) in the AWS Documentation.\n",
    "\n",
    "The target time series is the historical time series data for each item or product that's sold by the retail organization. The following fields are required:\n",
    "\n",
    "- **item_id** (string) – A unique identifier for the item or product that you want to predict the demand for.\n",
    "- **timestamp** (timestamp)\n",
    "- **demand** (float) – The number of sales for that item at the timestamp. It's also the target field that Amazon Forecast generates a forecast for.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you examine the previous data, there are certain columns that you don't need for your investigation. You can drop these columns. The columns you can drop are **Invoice**, **Description**, and **Customer ID**. \n",
    "\n",
    "**Note:** It's possible that items in the same order (as shown by the **Invoice** column) could have a correlation that impacts the model. For this lab, you will ignore this possibility.\n",
    "\n",
    "Drop the columns that you don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = retail[['StockCode','Quantity','Price','Country','InvoiceDate']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **InvoiceDate** column is your datetime data. You can inform pandas of this by using the `to_datetime` function. You can explore the data by time by setting the index of the DataFrame to the **InvoiceDate** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail['InvoiceDate'] = pd.to_datetime(retail.InvoiceDate)\n",
    "retail = retail.set_index('InvoiceDate')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now examine the updated DataFrame.\n",
    "\n",
    "The number of rows and columns are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data looks like this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that **InvoiceDate** is the index, and it's shown in the first column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you set the index to your datetime data, you can use it to select data.\n",
    "\n",
    "To select all the rows from a specific date, use the date in the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail['2010-01-04']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use parts of a date, and date ranges. To view the **Jan** and **Feb** rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail['2010-01':'2010-02']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date range starts at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.index.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date range ends at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.index.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pandas, you can extract date information easily. You might extract date information to explore the data further and look for time-related trends.\n",
    "\n",
    "Extract the year, month, and day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail['Year'] = retail.index.year\n",
    "retail['Month'] = retail.index.month\n",
    "retail['weekday_name'] = retail.index.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset that you now have includes purchases made between December 2009 and December 2010. It's reasonable to assume there would be some seasonality in this data. You will now investigate whether there is seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Month.value_counts(sort=False).plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart, you could deduce some seasonality:\n",
    "\n",
    "1. November and December seem to be higher than the rest of the year.\n",
    "\n",
    "2. Q4 seems to be higher than other quarters.\n",
    "\n",
    "3. For Q1, Q2, and Q3: The last month of the quarter (months 3, 6, and 9) seem to have spikes.\n",
    "\n",
    "Do you notice any other seasonal patterns?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, investigate whether there is any seasonality during the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "retail.weekday_name.value_counts(sort=False).loc[day_order].plot(kind='bar')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saturday shows very few orders. Why might this be the case?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Cleaning and reducing the size of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will reduce the size of the data. You will also remove any anomalies, such as negative prices, outliers, and country data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the countries\n",
    "Examine the **Country** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Country.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the data seems to be for the United Kingdom. To make your job easier, filter the data by *United Kingdom*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_filter = ['United Kingdom']\n",
    "retail = retail[retail.Country.isin(country_filter)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the **Country** column only contains the same value, you can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = retail[['StockCode','Quantity','Price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining StockCode and removing anomalies\n",
    "\n",
    "Examine the distribution of the **StockCode** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.StockCode.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4,015 unique values for **StockCode**. A quick plot of the counts might give you some insight into how the values are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.StockCode.value_counts().plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are a few high-selling products, with a long tail behind them. You could investigate this situation further. However, for now, examine **Quantity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Quantity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Quantity.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial plot, notice a couple of interesting aspects.\n",
    "\n",
    "1. There appear to be negative quantities.\n",
    "\n",
    "2. There are very large spikes throughout the year.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative and zero quantities could impact the forecast if you don't know why these values exist. To make things easier for now, you will remove negative and zero quantities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = retail[retail.Quantity>0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, examine **Price**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Price.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Price.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows some clear price spikes. You will now try to find out why these spikes exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail[retail.Price>500].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **StockCode** value of *M* looks unusual. If you had access to a domain expert, you could learn about the importance of *M*. Because you can't ask a domain expert for this lab, you will drop everything that has a **StockCode** value of *M*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = retail[retail.StockCode!='M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Price.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is better, but the **max** value is still high. You will now investigate this situation further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail[retail.Price>300].head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some adjustments occurred. You will also drop any data that shows these adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockcodes = ['ADJUST', 'ADJUST2', 'POST']\n",
    "retail = retail[~retail.StockCode.isin(stockcodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail.Price.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now examine zero-priced items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail[retail.Price==0].count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many values in these results, so you can drop zero-priced items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = retail[retail.Price>0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timeseries data that you need to create a forecast requires a *timestamp*, an *itemId*, and a *demand*. These features will map to the **InvoiceDate**, **StockCode**, and **Quantity** columns.\n",
    "\n",
    "The related timeseries data needs a *timestamp*, an *itemId*, and a *price*. These features will map to the **InvoiceDate**, **StockCode**, and **Price** columns.\n",
    "\n",
    "Create the two DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = retail[['StockCode','Quantity']]\n",
    "df_related_time_series = retail[['StockCode','Price']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now examine a single item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series[df_time_series.StockCode==21232]['2009-12-01']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see multiple orders for each day. You want to create a forecast that predicts demand at a daily level.\n",
    "\n",
    "You must *downsample* the data from the individual orders into a daily total.\n",
    "\n",
    "The orders for each day can be summed, because the total demand for the day is the value that you will forecast.\n",
    "\n",
    "pandas provides the `resample` function for this purpose. `sum` will sum the **Quantity** column. You will also reset the index based on the **InvoiceDate** value. However, this time, it will be a date without the time portion.\n",
    "\n",
    "**Note:** It might take up to 1 minute for this process to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = df_time_series.groupby('StockCode').resample('D').sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series['InvoiceDate'] = pd.to_datetime(df_time_series.InvoiceDate)\n",
    "df_time_series = df_time_series.set_index('InvoiceDate')\n",
    "df_time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series = df_time_series.groupby('StockCode').resample('D').sum().reset_index().set_index(['InvoiceDate'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series[df_time_series.StockCode==21232]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order now has a single entry for each day."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this process with the related time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related_time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related_time_series2 = df_related_time_series.groupby('StockCode').resample('D').mean().reset_index().set_index(['InvoiceDate','StockCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related_time_series2.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why are some of the previous values showing as *NaN*?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** That product had no orders for those days, and thus it has no price. Should you fill these NaN values with a numerical value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retail[retail.StockCode == 10002]['2009-12']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `pad` to forward-fill the price. The previous value will be used to fill the gap for each missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related_time_series3 = df_related_time_series2.groupby('StockCode').pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_related_time_series3.head(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Reviewing the creation of the forecast\n",
    "\n",
    "The following cells are Markdown. They demonstrate the API calls that are needed to create a forecast based on the data that you have been working with. Creating a forecast with Amazon Forecast involves three stages:\n",
    "\n",
    "1. Creating the datasets and importing the data. This process typically takes 5–10 minutes.\n",
    "2. Creating the predictor. This process trains a model by using the data that you provided. It takes 30–60 minutes to complete.\n",
    "3. Creating the forecast. This process generates a forecast for a particular item by using the predictor. It also takes 30–60 minutes to complete.\n",
    "\n",
    "To save time, when this lab was started, the `forecast-autorun.ipynb` was also ran in the background. The notebook will be updated with the results after running completes. It takes about 65 minutes to run, but it might take a little longer. By the time you review this cell, the forecast creation should in process. While it's finishing, you will review the code.\n",
    "\n",
    "**Note:** Feel free to review the actual `forecast-autorun.ipynb` notebook if you want some more detail. However, make sure that you don't run any cells!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the datasets and importing the data\n",
    "\n",
    "The first step is to create a Forecast Dataset Group:\n",
    "\n",
    "```python\n",
    "session = boto3.Session()\n",
    "forecast = session.client(service_name='forecast') \n",
    "create_dataset_group_response = forecast.create_dataset_group(DatasetGroupName=dataset_group_name, Domain=\"RETAIL\")\n",
    "dataset_group_arn = create_dataset_group_response['DatasetGroupArn']\n",
    "```\n",
    "    \n",
    "The `create_dataset` function requires a few parameters:\n",
    "\n",
    "- **DOMAIN** – This parameter specifies the domain, such as *retail*, that the forecast should use.\n",
    "- **DatasetType** – For the time series data, this parameter will be set to *TARGET_TIME_SERIES*.\n",
    "- **DatasetName** – This parameter specifies the name of the dataset.\n",
    "- **DataFrequency** – This parameter specifices the frequency. For the daily dataset, it will be *D*.\n",
    "- **Schema** – This parameter specifies the schema of the dataset.\n",
    "\n",
    "The dataset schema for the time series data is:\n",
    "\n",
    "```python\n",
    "schema ={\n",
    "   \"Attributes\":[\n",
    "      {\n",
    "         \"AttributeName\":\"timestamp\",\n",
    "         \"AttributeType\":\"timestamp\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"item_id\",\n",
    "         \"AttributeType\":\"string\"\n",
    "      },\n",
    "      {\n",
    "         \"AttributeName\":\"demand\",\n",
    "         \"AttributeType\":\"float\"\n",
    "      }\n",
    "   ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "The code to create the dataset is:\n",
    "\n",
    "```python\n",
    "time_series_response=forecast.create_dataset(\n",
    "                    Domain=\"RETAIL\",\n",
    "                    DatasetType='TARGET_TIME_SERIES',\n",
    "                    DatasetName='retail_time_series_data',\n",
    "                    DataFrequency='D', \n",
    "                    Schema = schema\n",
    ")\n",
    "dataset_arn = time_series_response['DatasetArn']\n",
    "```\n",
    "    \n",
    "Now that the dataset is defined, a job is needed to import the data:\n",
    "\n",
    "```python\n",
    "ds_import_job_response=forecast.create_dataset_import_job(DatasetImportJobName='retail_import_job',\n",
    "                                                      DatasetArn=dataset_arn,\n",
    "                                                      DataSource= data_source,\n",
    "                                                      TimestampFormat=timestamp_format\n",
    "                                                     )\n",
    "```\n",
    "\n",
    "Note that the *data_source* is a path to the data that's stored in Amazon Simple Storage Service (Amazon S3).\n",
    "\n",
    "The final step is to add the dataset to the dataset group:\n",
    "\n",
    "```python\n",
    "forecast.update_dataset_group(DatasetGroupArn=dataset_group_arn, DatasetArns=[dataset_arn])\n",
    "```\n",
    "    \n",
    "\n",
    "The process of adding the related data or metadata is done in the same way: by  changing the names, schema, and dataset type. Although you have prepared this data, you won't use it in the predictor because the model wasn't impacted by the additional data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the predictor\n",
    "\n",
    "The next step is to create the predictor. The `create_predictor` command needs a few parameters:\n",
    "\n",
    "- **PredictorName** – This parameter specifies the name that you want to give the predictor.\n",
    "\n",
    "    ```python\n",
    "    predictor_name= prefix+'_deeparp_algo'\n",
    "    ```\n",
    "\n",
    "\n",
    "- **AlgorithmArn** – This parameter is the path to the algorithm that you want to use. In this example, you will use DeepAR+.\n",
    "\n",
    "    ```python\n",
    "    algorithm_arn = 'arn:aws:forecast:::algorithm/Deep_AR_Plus\n",
    "    ```\n",
    "\n",
    "\n",
    "- **EvaluationParameters** – This parameter enables you to specify the number and size of the back test windows. Recall from the module that this parameter controls the size and number of testing windows that are created from the data.\n",
    "\n",
    "    ```python\n",
    "    evaluation_parameters= {\"NumberOfBacktestWindows\": 1, \"BackTestWindowOffset\": 30}\n",
    "    ```\n",
    "\n",
    "\n",
    "- **ForecastHorizon** – How many units to forecast (in this case, the units are days).\n",
    "\n",
    "    ```python\n",
    "    forecast_horizon = 30\n",
    "    ```\n",
    "\n",
    "\n",
    "- **InputDataConfig** – This parameter specifies the data, along with optional vacation days.\n",
    "\n",
    "    ```python\n",
    "    input_data_config = {\"DatasetGroupArn\": dataset_group_arn, \"SupplementaryFeatures\": [ {\"Name\": \"holiday\",\"Value\": \"UK\"} ]}\n",
    "    ```\n",
    "\n",
    "\n",
    "- **FeaturizationConfig** – This parameter sets the frequency, but it can also be used to specify filling methods for data.\n",
    "\n",
    "    ```python\n",
    "    featurization_config= {\"ForecastFrequency\": dataset_frequency }\n",
    "    ```\n",
    "\n",
    "The code to create the predictor is:\n",
    "\n",
    "```python\n",
    "create_predictor_response=forecast.create_predictor(PredictorName = predictor_name,\n",
    "      AlgorithmArn = algorithm_arn,\n",
    "      ForecastHorizon = forecast_horizon,\n",
    "      PerformAutoML = False,\n",
    "      PerformHPO = False,\n",
    "      EvaluationParameters= evaluation_parameters, \n",
    "      InputDataConfig = input_data_config,\n",
    "      FeaturizationConfig = featurization_config\n",
    "     )\n",
    "```\n",
    "                                                 \n",
    "After the predictor is created, you can create a forecast."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the forecast\n",
    "\n",
    "To create the forecast, use the `create_forecast` method:\n",
    "\n",
    "```python\n",
    "predictor_arn = create_predictor_response['PredictorArn']\n",
    "\n",
    "create_forecast_response=forecast.create_forecast(ForecastName=forecast_Name,\n",
    "                                                  PredictorArn=predictor_arn)\n",
    "\n",
    "```\n",
    "\n",
    "After the forecast is generated, the results can be queried by using the `query_forecast` method:\n",
    "\n",
    "```python\n",
    "forecast_response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn,\n",
    "    Filters={\"item_id\":\"22423\"}\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Waiting for the forecast creation to complete\n",
    "\n",
    "The forecast should now be created. You can investigate to see whether the forecast creation is complete.\n",
    "\n",
    "First, create a helper method to show the status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "class StatusIndicator:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.previous_status = None\n",
    "        self.need_newline = False\n",
    "        \n",
    "    def update( self, status ):\n",
    "        if self.previous_status != status:\n",
    "            if self.need_newline:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.write( status + \" \")\n",
    "            self.need_newline = True\n",
    "            self.previous_status = status\n",
    "        else:\n",
    "            # sys.stdout.write(\".\")\n",
    "            print('.',end='')\n",
    "            self.need_newline = True\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def end(self):\n",
    "        if self.need_newline:\n",
    "            sys.stdout.write(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create instances of the forecast and the forecast query objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='mlf-lab4-forecastbucket-12sb9sjex9iv'\n",
    "\n",
    "session = boto3.Session() \n",
    "forecast = session.client(service_name='forecast') \n",
    "forecast_query = session.client(service_name='forecastquery')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will read the variables from the store, and check whether the forecast was defined. After the forecast is defined, you will wait until its status becomes active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waiting for the predictor arn to be available')\n",
    "while True:\n",
    "    %store -r\n",
    "    is_local = \"forecast_arn\" in locals()\n",
    "    if is_local: break\n",
    "    print('.',end='')\n",
    "    time.sleep(10)\n",
    "\n",
    "print('Waiting for the predictor to be available')\n",
    "status_indicator_predictor = StatusIndicator()\n",
    "while True:\n",
    "    status = forecast.describe_predictor(PredictorArn=predictor_arn)['Status']\n",
    "    status_indicator_predictor.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator_predictor.end()\n",
    "    \n",
    "print('Waiting for forecast to be available')\n",
    "status_indicator = StatusIndicator()\n",
    "while True:\n",
    "    status = forecast.describe_forecast(ForecastArn=forecast_arn)['Status']\n",
    "    status_indicator.update(status)\n",
    "    if status in ('ACTIVE', 'CREATE_FAILED'): break\n",
    "    time.sleep(10)\n",
    "\n",
    "status_indicator.end()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Using the forecast\n",
    "\n",
    "At this point, there should be a forecast that's ready to be queried.\n",
    "\n",
    "Check that you get data for the following test stock code: *21232*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "forecast_response = forecast_query.query_forecast(\n",
    "    ForecastArn=forecast_arn,\n",
    "    Filters={\"item_id\":\"21232\"}\n",
    ")\n",
    "print(forecast_response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the actual results\n",
    "\n",
    "Earlier, you split the data and held back the *November* and *December* values. You will plot these values against the predicted values for the same time period.\n",
    "\n",
    "You will start by reading the test values back into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df = pd.read_csv(test, names=['InvoiceDate','StockCode','Quantity'])\n",
    "actual_df['InvoiceDate'] = pd.to_datetime(actual_df.InvoiceDate)\n",
    "actual_df = actual_df.set_index('InvoiceDate')\n",
    "actual_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that you only have data for the *21232* stock code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockcode_filter = ['21232']\n",
    "actual_df = actual_df[actual_df['StockCode'].isin(stockcode_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do a quick plot of the data. Remember that this data is test data, so the actual values are plotted. In the next step, you will plot the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df.Quantity.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the prediction\n",
    "\n",
    "Next, you must convert the JSON response from the predictor to a DataFrame that you can plot.\n",
    "\n",
    "Start by getting the P10 predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DF \n",
    "prediction_df_p10 = pd.DataFrame.from_dict(forecast_response['Forecast']['Predictions']['p10'])\n",
    "prediction_df_p10.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, plot the P10 predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "prediction_df_p10.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code only retrieved the P10 values and put them in a DataFrame. Now, complete the same process for the P50 and P90 values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df_p50 = pd.DataFrame.from_dict(forecast_response['Forecast']['Predictions']['p50'])\n",
    "prediction_df_p90 = pd.DataFrame.from_dict(forecast_response['Forecast']['Predictions']['p90'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Comparing the prediction to actual results\n",
    "\n",
    "After you obtain the DataFrames, the next task is to plot them together to determine the best fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by creating a DataFrame to house the content. Here, Source will be which DataFrame it came from.\n",
    "results_df = pd.DataFrame(columns=['timestamp','value','Source'])\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Import the observed values into the DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "for index, row in actual_df.iterrows():\n",
    "    #clean_timestamp = dateutil.parser.parse(index)\n",
    "    results_df = results_df.append({'timestamp' : index , 'value' : row['Quantity'], 'Source': 'Actual'} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show the new DataFrame\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now add the P10, P50, and P90 Values\n",
    "for index, row in prediction_df_p10.iterrows():\n",
    "    clean_timestamp = dateutil.parser.parse(row['Timestamp'])\n",
    "    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'Source': 'p10'} , ignore_index=True)\n",
    "for index, row in prediction_df_p50.iterrows():\n",
    "    clean_timestamp = dateutil.parser.parse(row['Timestamp'])\n",
    "    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'Source': 'p50'} , ignore_index=True)\n",
    "for index, row in prediction_df_p90.iterrows():\n",
    "    clean_timestamp = dateutil.parser.parse(row['Timestamp'])\n",
    "    results_df = results_df.append({'timestamp' : clean_timestamp , 'value' : row['Value'], 'Source': 'p90'} , ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating a pivot on the data, you can compare the actual P10, P50, and P90 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = results_df.pivot(columns='Source', values='value', index=\"timestamp\")\n",
    "pivot_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charts can be easier to analyze than the raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df.plot(figsize=(20,10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the results\n",
    "\n",
    "Hopefully, in the previous chart, you will see at least some correlation between the predicted values and the actual values. The correlation might not be good, and there could be several reasons for this outcome:\n",
    "\n",
    "- The sales are mostly wholesale, but they do include some smaller orders.\n",
    "- You held back data, which meant that an entire season wasn't included in the training data.\n",
    "- You might have been missing useful category or sales promotion data.\n",
    "\n",
    "Like all machine learning models, the results are as good as the data you use to train the model. As noted previously, the model could be improved with more data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Cleaning up\n",
    "\n",
    "The following cells will clean up the resources that were created during the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.delete_forecast(ForecastArn=forecast_arn)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.delete_predictor(PredictorArn=predictor_arn)\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.delete_dataset_import_job(DatasetImportJobArn=ds_related_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.delete_dataset_import_job(DatasetImportJobArn=ds_import_job_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.delete_dataset(DatasetArn=related_dataset_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.delete_dataset(DatasetArn=dataset_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.delete_dataset_group(DatasetGroupArn=dataset_group_arn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
